{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MegaDetector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from PytorchWildlife.models import detection as pw_detection\n",
    "\n",
    "# Path to the MegaDetector code directory #No longer used\n",
    "md_code_path = \"CameraTraps/detection\"\n",
    "\n",
    "# Path to the model we downloaded #No longer used\n",
    "model_path = \"md_v5a.0.pb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.74  Python-3.11.2 torch-2.6.0+cpu CPU (Intel Core(TM) i7-10750H 2.60GHz)\n",
      "YOLOv9c summary (fused): 384 layers, 25,321,561 parameters, 0 gradients, 102.3 GFLOPs\n",
      "\n",
      "0: 480x640 1 animal, 503.6ms\n",
      "Speed: 5.0ms preprocess, 503.6ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Error converting bounding box: 'str' object has no attribute 'tolist'\n",
      "Type of bbox: <class 'str'>\n",
      "Could not convert confidence score d to float\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'item'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 67\u001b[0m\n\u001b[0;32m     64\u001b[0m     plt\u001b[38;5;241m.\u001b[39mclose(fig)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m# Run MegaDetector on the image\u001b[39;00m\n\u001b[1;32m---> 67\u001b[0m detections \u001b[38;5;241m=\u001b[39m \u001b[43mrun_megadetector_on_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# Draw bounding boxes and save the output image\u001b[39;00m\n\u001b[0;32m     70\u001b[0m draw_boxes(image_path, detections, output_path)\n",
      "Cell \u001b[1;32mIn[4], line 38\u001b[0m, in \u001b[0;36mrun_megadetector_on_image\u001b[1;34m(image_path)\u001b[0m\n\u001b[0;32m     32\u001b[0m              \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not convert confidence score \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfidence\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to float\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;66;03m#See the value of confidence\u001b[39;00m\n\u001b[0;32m     33\u001b[0m              confidence_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;66;03m# Fallback value, change to appropriate score\u001b[39;00m\n\u001b[0;32m     35\u001b[0m         detections[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m\"\u001b[39m][image_path]\u001b[38;5;241m.\u001b[39mappend({\n\u001b[0;32m     36\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbbox\u001b[39m\u001b[38;5;124m\"\u001b[39m: bbox_list,\n\u001b[0;32m     37\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfidence\u001b[39m\u001b[38;5;124m\"\u001b[39m: confidence_val,\n\u001b[1;32m---> 38\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(\u001b[43mcategory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m()), \u001b[38;5;66;03m# if category is a tensor\u001b[39;00m\n\u001b[0;32m     39\u001b[0m         })\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m detections\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'item'"
     ]
    }
   ],
   "source": [
    "# Input image path\n",
    "image_path = 'D:/orinoquia_camera_traps_images/A06/100EK113/01210093.JPG'\n",
    "# Output path for the image with bounding box\n",
    "output_path = \"detection_output.jpg\"\n",
    "\n",
    "# Function to run MegaDetector on a single image\n",
    "def run_megadetector_on_image(image_path):\n",
    "    detection_model = pw_detection.MegaDetectorV6(version=\"MDV6-yolov9-c\") # Model weights are automatically downloaded.\n",
    "\n",
    "    img = np.array(Image.open(image_path))\n",
    "    detection_results = detection_model.single_image_detection(img)\n",
    "\n",
    "    detections = {}\n",
    "    detections[\"images\"] = {}\n",
    "    detections[\"images\"][image_path] = []\n",
    "\n",
    "    # Format the results as MegaDetector output\n",
    "    if len(detection_results) > 0: # Make sure there are not detections, to avoid errors\n",
    "        boxes, scores, classes = detection_results\n",
    "        for bbox, confidence, category in zip(boxes, scores, classes):\n",
    "            try:\n",
    "                bbox_list = bbox.tolist()\n",
    "            except AttributeError as e:\n",
    "                print(f\"Error converting bounding box: {e}\")\n",
    "                print(f\"Type of bbox: {type(bbox)}\")  # See the type of object bbox\n",
    "                bbox_list = bbox # Keep the object \"as is\" \n",
    "\n",
    "            # convert confidence score for correct output\n",
    "            try:\n",
    "                confidence_val = float(confidence) # Convert confidence score\n",
    "            except ValueError as e:\n",
    "                 print(f\"Could not convert confidence score {confidence} to float\") #See the value of confidence\n",
    "                 confidence_val = 0 # Fallback value, change to appropriate score\n",
    "\n",
    "            detections[\"images\"][image_path].append({\n",
    "                \"bbox\": bbox_list,\n",
    "                \"confidence\": confidence_val,\n",
    "                \"category\": str(category.item()), # if category is a tensor\n",
    "            })\n",
    "\n",
    "    return detections\n",
    "\n",
    "# Helper function to draw bounding boxes on images\n",
    "def draw_boxes(image_path, detections, output_path):\n",
    "    img = np.array(Image.open(image_path))\n",
    "\n",
    "    fig, ax = plt.subplots(1)\n",
    "    ax.imshow(img)\n",
    "    # Plot the bounding boxes\n",
    "    if(image_path in detections[\"images\"]):\n",
    "        for detection in detections[\"images\"][image_path]:\n",
    "              bbox = detection['bbox']\n",
    "              confidence = detection['confidence']\n",
    "              # Scale bounding box coordinates from [y1, x1, y2, x2] to image size\n",
    "              height, width, _ = img.shape\n",
    "              x1 = int(bbox[1] * width)\n",
    "              y1 = int(bbox[0] * height)\n",
    "              x2 = int(bbox[3] * width)\n",
    "              y2 = int(bbox[2] * height)\n",
    "              rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=2, edgecolor='r', facecolor='none')\n",
    "              ax.add_patch(rect)\n",
    "              ax.text(x1, y1 - 5, f'{confidence:.2f}', color='white', fontsize=8, backgroundcolor='r')\n",
    "    plt.savefig(output_path)\n",
    "    plt.close(fig)\n",
    "\n",
    "# Run MegaDetector on the image\n",
    "detections = run_megadetector_on_image(image_path)\n",
    "\n",
    "# Draw bounding boxes and save the output image\n",
    "draw_boxes(image_path, detections, output_path)\n",
    "\n",
    "print(f\"Saved detection to {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
